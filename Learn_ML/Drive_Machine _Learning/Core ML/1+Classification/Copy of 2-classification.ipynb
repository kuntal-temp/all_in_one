{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOQ8rfYbm7aW"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_kpZDaxm6xe"
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "params = {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, \n",
    "          'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', \n",
    "          'random_state': 0, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
    "classifier.set_params(**params)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# multinomial\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10)\n",
    "softmax_reg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mZi5Ld-o1pi"
   },
   "source": [
    "# SGD Classifier\n",
    "\n",
    "> SGD Classifier is a linear classifier (SVM, logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kj2C4xfpEzu"
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jd1h479ZpGcM"
   },
   "source": [
    "# LinearSVC\n",
    "- penalty{‘l1’, ‘l2’}\n",
    "- loss{‘hinge’, ‘squared_hinge’}, default=’squared_hinge’ [penalty='l1' and loss='hinge' is not supported]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ClWD4y7pV_h"
   },
   "outputs": [],
   "source": [
    "reg = LinearSVC(random_state=0, tol=1e-5)\n",
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emMAWaC0nAtO"
   },
   "source": [
    "# Support Vector Classifier\n",
    "\n",
    "- kernel: linear, rbf, poly, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GN8olsbdmpZr"
   },
   "outputs": [],
   "source": [
    "classifier = SVC()\n",
    "\n",
    "# kernel=\"rbf\"\n",
    "params = {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, \n",
    "          'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, \n",
    "          'probability': False, 'random_state': 0, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
    "\n",
    "# kernel=\"linear\"\n",
    "params = {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, \n",
    "          'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'linear', \n",
    "          'max_iter': -1, 'probability': False, 'random_state': 0, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
    "\n",
    "classifier.set_params(**params)\n",
    "\n",
    "# kernel=\"poly\"\n",
    "classifier = SVC(kernel=\"poly\", degree=3, coef0=1, C=5)\n",
    "\n",
    "# kernel ='sigmoid'\n",
    "classifier = SVC(kernel ='sigmoid')\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WkJ6fvXn3Nd"
   },
   "source": [
    "# K-Neighbors Classifier\n",
    "\n",
    "- weights: {‘uniform’, ‘distance’}\n",
    "    1. ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally\n",
    "    2. ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away\n",
    "\n",
    "- algorithm: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}. Algorithm used to compute the nearest neighbors\n",
    "\n",
    "- leaf_size: Leaf size passed to BallTree or KDTree\n",
    "\n",
    "- p: Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7nXB5Jxm4GO"
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "classifier = KNeighborsClassifier()\n",
    "params = {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, \n",
    "          'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
    "classifier.set_params(**params)\n",
    "\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOkGTB34oDQm"
   },
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "- criterion: {“gini”, “entropy”}. The function to measure the quality of a split\n",
    "- splitter: {“best”, “random”}. The strategy used to choose the split at each node\n",
    "- max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples\n",
    "- min_samples_split: The minimum number of samples required to split an internal node\n",
    "-min_samples_leaf: The minimum number of samples required to be at a leaf node\n",
    "- max_features: The number of features to consider when looking for the best split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJl9UL_FoEUo"
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier()   # CART cost function\n",
    "params = {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': None, \n",
    "          'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, \n",
    "          'min_weight_fraction_leaf': 0.0, 'random_state': 0, 'splitter': 'best'}\n",
    "classifier.set_params(**params)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# Visualize the trained tree\n",
    "tree.plot_tree(classifier);\n",
    "\n",
    "\"\"\"\n",
    "  OR\n",
    "\"\"\"\n",
    "\n",
    "fn=['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']\n",
    "cn=['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "export_graphviz(clf,\n",
    "    out_file=\"tree.dot\",\n",
    "    feature_names = fn, \n",
    "    class_names=cn,\n",
    "    filled = True\n",
    ")\n",
    "\n",
    "export_graphviz(\n",
    "    classifier,\n",
    "    out_file=image_path(\"view_tree.dot\"),\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_name=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "#> dot -Tpng view_tree.dot -o view_tree.png     [run it to convert from .dot to .png]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TmaOA89oTfH"
   },
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7-Ts8hboUYZ"
   },
   "outputs": [],
   "source": [
    "classifier = GaussianNB()\n",
    "params = {'priors': None, 'var_smoothing': 1e-09}\n",
    "classifier.set_params(**params)\n",
    "\n",
    "# BernoulliNB\n",
    "classifier = BernoulliNB()\n",
    "\n",
    "# MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# CategoricalNB\n",
    "classifier = CategoricalNB()\n",
    "\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDcarv0MofxC"
   },
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZvnBjdZogrt"
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "params = {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, \n",
    "          'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, \n",
    "          'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, \n",
    "          'n_jobs': None, 'oob_score': False, 'random_state': 0, 'verbose': 0, 'warm_start': False}\n",
    "classifier.set_params(**params)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# The following code trains a Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using all available CPU cores\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBD-J4D-pkS2"
   },
   "source": [
    "# Bagging Classifier\n",
    "\n",
    "> performs soft voting\n",
    "\n",
    "- base_estimator: The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a DecisionTreeRegressor\n",
    "- n_estimators: The number of base estimators in the ensemble\n",
    "- max_samples: The number of samples to draw from X to train each base estimator\n",
    "- max_features: The number of features to draw from X to train each base estimator\n",
    "- bootstrap: Whether samples are drawn with replacement. If False, sampling without replacement is performed\n",
    "- oob_score: Whether to use out-of-bag samples to estimate the generalization error. Only available if bootstrap=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1xfX_mqo40V"
   },
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(\n",
    "    base_estimator=SVC(), n_estimators=10,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "clf.fit(X, y)\n",
    "clf.clf.predict(X)\n",
    "\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, max_samples=100, \n",
    "    bootstrap=True, n_jobs=-1\n",
    ")\n",
    "\n",
    "bag_clf.fit(X, y)\n",
    "\n",
    "print(\"oob score = \", bag_clf.oob_score_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzR5U2t4py4v"
   },
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwNolYNVp128"
   },
   "outputs": [],
   "source": [
    "# Example 1\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('clf1', log_clf), ('clf2', rnd_clf), ('clf3', svm_clf)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "\n",
    "# Example 2\n",
    "clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "eclf1 = eclf1.fit(X, y)\n",
    "print(eclf1.predict(X))\n",
    "\n",
    "eclf2 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n",
    "eclf2 = eclf2.fit(X, y)\n",
    "print(eclf2.predict(X))\n",
    "\n",
    "eclf3 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft', weights=[2,1,1], flatten_transform=True)\n",
    "eclf3 = eclf3.fit(X, y)\n",
    "print(eclf3.predict(X))\n",
    "print(eclf3.transform(X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzZpgmeAp_v3"
   },
   "source": [
    "# Stacking [Stacked Generalization]\n",
    "\n",
    "> Blending is an ensemble machine learning technique that uses a machine learning model to learn how to best combine the predictions from multiple contributing ensemble member models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nFgEjldqCs-"
   },
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "estimators=[\n",
    "    ('clf1', log_clf), ('clf2', rnd_clf), ('clf3', svm_clf)\n",
    "]\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUgGfaBoqQEB"
   },
   "source": [
    "## Gradient Boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5djun9lqRJw"
   },
   "outputs": [],
   "source": [
    "classifier = GradientBoostingClassifier()\n",
    "params = {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 1.0, 'loss': 'deviance', 'max_depth': 1, \n",
    "          'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, \n",
    "          'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 0, 'subsample': 1.0, \n",
    "          'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
    "classifier.set_params(**params)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzEOOfebqYA3"
   },
   "source": [
    "# Hist Gradient Boosting Classifier\n",
    "\n",
    "Histogram-based Gradient Boosting Classification Tree\n",
    "\n",
    "> This estimator is much faster than GradientBoostingClassifier for big datasets (n_samples >= 10 000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QRHl2E0qbAV"
   },
   "outputs": [],
   "source": [
    "clf = HistGradientBoostingClassifier()\n",
    "clf.fit(X, y)\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNYTFHxfqo7s"
   },
   "source": [
    "# XGB Classifier [Extreme Gradient Boosting]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81upTlQWqp8s"
   },
   "outputs": [],
   "source": [
    "my_model = XGBClassifier()\n",
    "\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCXsGsHVq0jH"
   },
   "source": [
    "# Adaptive Boosting\n",
    "\n",
    "- base_estimator: If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1\n",
    "- algorithm: {‘SAMME’, ‘SAMME.R’} \n",
    "If ‘SAMME.R’ then use the SAMME.R real boosting algorithm. base_estimator must support calculation of class probabilities. If ‘SAMME’ then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_Hg95-iq6Vl"
   },
   "outputs": [],
   "source": [
    "classifier = AdaBoostClassifier()\n",
    "params = {'algorithm': 'SAMME.R', 'base_estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': None}\n",
    "classifier.set_params(**params)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqpRcAHErfFQ"
   },
   "source": [
    "# Light GBM [LGBM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2fFdnO8rhrJ"
   },
   "outputs": [],
   "source": [
    "model = LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=42)\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train, eval_set=[(x_test,y_test), (x_train,y_train)],\n",
    "    verbose=20, eval_metric='logloss'\n",
    ")\n",
    "\n",
    "\n",
    "# Extra\n",
    "lgb.plot_importance(model)\n",
    "lgb.plot_metric(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZngFjFfrmez"
   },
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38y5r-Scro8x"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "categorical_features_indices = np.where(X.dtypes != np.float)[0]\n",
    "\n",
    "cls = CatBoostClassifier(iterations=5, learning_rate=0.1, loss_function='MultiClass')\n",
    "\n",
    "cls.fit(X_train, y_train, \n",
    "        cat_features=categorical_features_indices, \n",
    "        eval_set=(X_val, y_val), \n",
    "        verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHrOrH3hruSZ"
   },
   "source": [
    "# Score analysis\n",
    "\n",
    "### Precision vs. Recall\n",
    "- False Positive (FP) – Type 1 error\n",
    ">The predicted value was falsely predicted\n",
    "The actual value was negative but the model predicted a positive value\n",
    "Also known as the Type 1 error\n",
    "\n",
    "- False Negative (FN) – Type 2 error\n",
    ">The predicted value was falsely predicted\n",
    "The actual value was positive but the model predicted a negative value\n",
    "Also known as the Type 2 error\n",
    "\n",
    "### Precision\n",
    ">It tells us how many of the correctly predicted cases actually turned out to be positive. <br/>Precision is a useful metric in cases where False Positive is a higher concern than False Negatives.\n",
    "```\n",
    "  precision = TP/(TP+FP) => 0.5\n",
    "```\n",
    "\n",
    "### Recall \n",
    ">It tells us how many of the actual positive cases we were able to predict correctly with our model. <br/>Recall is a useful metric in cases where False Negative trumps False Positive.\n",
    "```\n",
    "  recall = TP/(TP+FN) => 0.75\n",
    "```\n",
    "\n",
    "### Precision Recall Tradeoff\n",
    ">If you increase precision, it will reduce recall and vice versa. This is called the precision/recall tradeoff.\n",
    "\n",
    "### F1-Score\n",
    ">when we try to increase the precision of our model, the recall goes down, and vice-versa. The F1-score captures both the trends in a single value. F1-score is a harmonic mean of Precision and Recall. <br/>The interpretability of the F1-score is poor. This means that we don’t know what our classifier is maximizing – precision or recall? So, we use it in combination with other evaluation metrics which gives us a complete picture of the result.\n",
    "```\n",
    "  f1_score = 2/((1/recall)+(1/precision))\n",
    "```\n",
    "\n",
    "### ROC Curve\n",
    ">ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. Best value = 1\n",
    "\n",
    "### AUC Curve\n",
    "> The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes\n",
    "\n",
    "\n",
    "Ref Link: https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vkfi51cvr09b"
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "tp, fn, fp, tn = confusion_matrix(Y_test, Y_pred).reshape(-1)\n",
    "con_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "print(\"confusion matrix = \", con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blgnAu-Gr1gZ"
   },
   "outputs": [],
   "source": [
    "# precision_score\n",
    "precision_score = precision_score(Y_test, Y_pred)\n",
    "\n",
    "# recall_score\n",
    "recall_score = recall_score(Y_test, Y_pred)\n",
    "\n",
    "# F1 score\n",
    "f1_score = f1_score(Y_test, Y_pred)\n",
    "\n",
    "# classification_report\n",
    "print(classification_report(actual,predicted))\n",
    "\n",
    "# precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, Y_pred)\n",
    "plt.plot(thresholds, precision[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recall[:-1], \"g-\", label=\"Recall\")\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.legend()\n",
    "\n",
    "# ROC\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_pred)\n",
    "plt.plot(fpr, tpr, lw=2)\n",
    "plt.xlabel(\"Flase Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "# roc_auc_score\n",
    "roc_auc_score = roc_auc_score(Y_test, Y_pred)\n",
    "\n",
    "# score\n",
    "score = classifier.score(X_test, Y_pred)\n",
    "print(\"score = \", score)\n",
    "\n",
    "# accuracy score\n",
    "acc_score = accuracy_score(Y_test, Y_pred)\n",
    "print(\"accuracy score = \", acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDTTr9dhr7ax"
   },
   "outputs": [],
   "source": [
    "# zero one loss\n",
    "\"\"\"\n",
    "  If normalize is True, return the fraction of misclassifications (float), \n",
    "  else it returns the number of misclassifications (int). \n",
    "  The best performance is 0.\n",
    "  \n",
    "  normalize = True/False\n",
    "\"\"\"\n",
    "zero_one_loss(Y_test, Y_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNephv8qr-rj"
   },
   "source": [
    "## Compare between Actual and Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1V2OKkkpsA8P"
   },
   "outputs": [],
   "source": [
    "compare_predict_data = np.concatenate((Y_pred.reshape(len(Y_pred),1), Y_test.reshape(len(Y_test),1)),1)[0:5]\n",
    "print(compare_predict_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ-OSdJOsDfk"
   },
   "source": [
    "## K-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMtpQuXcsGNm"
   },
   "outputs": [],
   "source": [
    "# Way 1\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = Y_train, cv = 5)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzO3wOk1sIeb"
   },
   "outputs": [],
   "source": [
    "# Way 2\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "accuracies = cross_val_score(classifier, X, y, cv=cv, n_jobs=-1)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7YCaaC5sLWy"
   },
   "source": [
    "## **Model Improvement**  \n",
    "### Using GridSearchCV and RandomizedSearchCV\n",
    "\n",
    "1. https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dcp6IkTnsOGv"
   },
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "# Logistic Regression\n",
    "\"\"\" [default params]\n",
    "params = {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, \n",
    "          'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', \n",
    "          'random_state': 0, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" [All scoring value for Regression]\n",
    "\n",
    "scoring = [\n",
    "  accuracy, balanced_accuracy, top_k_accuracy, average_precision, neg_brier_score, f1. f1_micro, f1_macro,\n",
    "  f1_weighted, f1_samples, neg_log_loss, roc_auc_ovo_weighted, roc_auc_ovo, roc_auc_ovr,  roc_auc, jaccard, recall, precision\n",
    "  ]\n",
    "\n",
    "penalty : ['l2', 'l1', 'elasticnet']\n",
    "solver : ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga']\n",
    "\"\"\"\n",
    "\n",
    "parameters = [\n",
    "  {\n",
    "    'C': [0.25, 0.5, 0.75, 1],\n",
    "    'fit_intercept': [True, False],\n",
    "    'intercept_scaling': [1, 2, 3, 4, 5],\n",
    "    'max_iter': [100],\n",
    "    'penalty': ['l2'],\n",
    "    'warm_start': [True, False],\n",
    "    'solver': ['lbfgs']\n",
    "  },\n",
    "  {\n",
    "    'C': [0.25, 0.5, 0.75, 1],\n",
    "    'fit_intercept': [True, False],\n",
    "    'intercept_scaling': [1, 2, 3, 4, 5],\n",
    "    'max_iter': [100],\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'warm_start': [True, False],\n",
    "    'solver': ['liblinear']\n",
    "  },\n",
    "  {\n",
    "    'C': [0.25, 0.5, 0.75, 1],\n",
    "    'fit_intercept': [True, False],\n",
    "    'intercept_scaling': [1, 2, 3, 4, 5],\n",
    "    'max_iter': [100],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'warm_start': [True, False],\n",
    "    'solver': ['saga']\n",
    "  }  \n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = 'accuracy', cv = 5)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niD27afIsQmJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "y_pred = [1, 2, 3, 4]\n",
    "y_true = [2, 0, 3, 4]\n",
    "zero_one_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJjuc7m9sSk1"
   },
   "outputs": [],
   "source": [
    "zero_one_loss(y_true, y_pred, normalize=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN4mhYj2AfIrZmq2powANFm",
   "collapsed_sections": [],
   "name": "Copy of 2-classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
