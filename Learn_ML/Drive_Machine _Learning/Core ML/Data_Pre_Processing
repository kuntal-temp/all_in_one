{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_Pre_Processing","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPQeNNMs8S8I1PxXv0mwyxL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Import All Libraries"],"metadata":{"id":"2dzMb8FvS_Mh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pl2DaFxqS9lg"},"outputs":[],"source":["import math, random\n","import PIL\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import scipy.cluster.hierarchy as sch\n","\n","import joblib\n","\n","from sklearn import datasets as skdata\n","from sklearn.metrics import mean_squared_error, r2_score, classification_report,confusion_matrix, accuracy_score, zero_one_loss\n","from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve, roc_curve, roc_auc_score, silhouette_score\n","from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\n","from sklearn.impute import SimpleImputer, KNNImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler, PolynomialFeatures\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, RepeatedKFold\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.decomposition import PCA, KernelPCA, IncrementalPCA\n","from sklearn.manifold import MDS, Isomap, TSNE, LocallyLinearEmbedding\n","from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, LogisticRegression, SGDRegressor, SGDClassifier\n","from sklearn.svm import SVR, SVC, LinearSVR\n","from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier, AdaBoostRegressor, AdaBoostClassifier, VotingRegressor, VotingClassifier, BaggingRegressor, BaggingClassifier, StackingClassifier\n","from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, DBSCAN, OPTICS, SpectralClustering, MiniBatchKMeans\n","from sklearn.pipeline import make_pipeline\n","\n","from xgboost import XGBClassifier, XGBRegressor, XGBRFClassifier, XGBRFRegressor\n","\n","from tensorflow.keras import datasets as tfdata\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import to_categorical\n","\n","# try:\n","#   from apyori import apriori\n","# except:\n","#   !pip install apyori\n","# finally:\n","#   from apyori import apriori\n","# try:\n","#   from umap import UMAP\n","# except:\n","#   !pip install umap-learn\n","# finally:\n","#   from umap import UMAP\n","\n","# Make NumPy printouts easier to read.\n","np.set_printoptions(precision=2, suppress=True)"]},{"cell_type":"markdown","source":["# Importing the dataset"],"metadata":{"id":"iMjwC5fbaWna"}},{"cell_type":"code","source":["dataset = pd.read_csv('Iris.csv')\n","dataset.head()"],"metadata":{"id":"_alzJrqYaYbw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dividing data into X and Y"],"metadata":{"id":"ug1nYsd5cTKu"}},{"cell_type":"code","source":["X = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, -1].values"],"metadata":{"id":"I4AiDHLocU-g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Analysis\n","\n","Ref Link:\n","1.  https://www.analyticsvidhya.com/blog/2021/04/mastering-exploratory-data-analysiseda-for-data-science-enthusiasts/\n","2.  \n","\n"],"metadata":{"id":"HBN_emBzabA4"}},{"cell_type":"code","source":["\"\"\"\n","Test Code\n","\"\"\"\n","\n","iris.groupby(‘species’).agg([‘mean’, ‘median’])\n","\n","sns.boxplot( y=”petal_width”, x= “species”, data=iris_data, orient=’v’ , ax=axes[0, 0])\n","\n","sns.violinplot( y=”petal_width”, x= “species”, data=iris_data, orient=’v’ , ax=axes[0, 0],inner=’quartile’)\n","\n","sns.FacetGrid(iris, hue=\"species\", height=5).map(sns.distplot, \"sepal_length\").add_legend()"],"metadata":{"id":"43rWOTwN3MEI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.info()"],"metadata":{"id":"arfAbZW8abLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This step should be performed for getting details about various statistical data like Mean, Standard Deviation, Median, Max Value, Min Value\n","dataset.describe().transpose()"],"metadata":{"id":"4-j5HAImazYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Removing Duplicates\n","\n","data.duplicated().sum()  # returning total number of duplicates entries\n","data.drop_duplicates(inplace=True)"],"metadata":{"id":"jkDqf2TOqU5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.pairplot(dataset)"],"metadata":{"id":"g35rXFjEa2pm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.hist(bins=100, figsize=(15, 10))  # figsize=(W, H)"],"metadata":{"id":"kI9gvwambSwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Correlation explains how one or more variables are related to each other.\n","If two variables are closely correlated, then we can predict one variable from the other.\n","\"\"\"\n","dataset.corr()"],"metadata":{"id":"bMnx5Bf7bm7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.heatmap(dataset.corr(), annot=True)"],"metadata":{"id":"K3QObguLb2Gg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Width & Height Relation View\n","\n","dataset.plot(kind=\"scatter\", x=\"Width\", y=\"Height\", figsize=(7, 5), alpha=0.2)\n","\"\"\"\n","  OR\n","\"\"\"\n","sns.scatterplot(x=\"Width\", y=\"Height\", data=dataset, alpha=0.2)"],"metadata":{"id":"lizUeyyDbWov"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# How to drop out Highly Correlated Features"],"metadata":{"id":"Vho23wNlb7pA"}},{"cell_type":"code","source":["df_X = pd.DataFrame(X)\n","corr_matrix = df_X.corr()\n","upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\n","threshold_val = 0.95\n","to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold_val)]\n","df_X = df_X.drop(df_X.columns[to_drop], axis=1)\n","df_X.head()\n","\n","# After removing columns\n","X = df_X.iloc[:, :].values"],"metadata":{"id":"t6TYQWLOcBhX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Taking care of missing data"],"metadata":{"id":"wKCdUTxkdibj"}},{"cell_type":"code","source":["imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","X[:, :] = imputer.fit_transform(X[:, :])\n","print(X[0:5])"],"metadata":{"id":"_3hW0-twdkoa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Encoding Independent categorical data"],"metadata":{"id":"pFocXuYlduQU"}},{"cell_type":"code","source":["ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n","X = ct.fit_transform(X)\n","print(X[0:5])"],"metadata":{"id":"6kELAeQBdvWV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Label Encoding"],"metadata":{"id":"6zbNzPP4dzbS"}},{"cell_type":"code","source":["le = LabelEncoder()\n","y = le.fit_transform(y)\n","print(y[0:5])"],"metadata":{"id":"CWgOewP-d7Yp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Test Spliting"],"metadata":{"id":"q-FX0Phxd9ua"}},{"cell_type":"code","source":["X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)"],"metadata":{"id":"2h-wlPCKeBm7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature Scaling\n","\n","- Why scalling?\n",">For example, assume your input dataset contains one column with values ranging from 0 to 1, and another column with values ranging from 10,000 to 100,000. The great difference in the scale of the numbers could cause problems when you attempt to combine the values as features during modeling\n","\n","### Type of Scaling\n","1. Standardization\n","2. Normalization\n","\n","- Standardization\n","> where mean=0 and std=1. \n","\n","- Normalization\n","> You can change all values to a 0 to 1 OR -1 to +1toscale\n","\n","\n","Note: Random forest, decision tree, xgboost does not required scaling\n","but for liner, polynomial regression it must required"],"metadata":{"id":"ej9eYEVzeFdq"}},{"cell_type":"code","source":["# Feature Scaling For X\n","sc_x = StandardScaler()\n","X_train[:, :] = sc_x.fit_transform(X_train[:, :])\n","X_test[:, :] = sc_x.fit_transform(X_test[:, :])"],"metadata":{"id":"lUZgzqryeH6a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature Scaling For Y\n","sc_y = StandardScaler()\n","Y_train = Y_train.reshape(-1,1)\n","Y_test = Y_test.reshape(-1,1)\n","Y_train[:, :] = sc_y.fit_transform(Y_train[:, :])\n","Y_test[:, :] = sc_y.transform(Y_test[:, :])\n","Y_train = Y_train.flatten(order='C')\n","Y_test = Y_test.flatten(order='C')"],"metadata":{"id":"fHFvw8EVeJiK"},"execution_count":null,"outputs":[]}]}