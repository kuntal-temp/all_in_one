{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKwij1U4nEvY"
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "- fit_intercept\n",
    "    > If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use StandardScaler before calling fit on an estimator with normalize=False\n",
    "\n",
    "- copy_X\n",
    "> If True, X will be copied; else, it may be overwritten\n",
    "\n",
    "- n_jobs\n",
    "> -1 means means using all processors\n",
    "\n",
    "- positive\n",
    "> When set to True, forces the coefficients to be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjzkZujBcgYf"
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "params = {'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'normalize': 'deprecated', 'positive': False}\n",
    "regressor.set_params(**params)\n",
    "regressor.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmMSPWlLnJUb"
   },
   "source": [
    "## Lasso Regression [L1]\n",
    "\n",
    "- alpha\n",
    "    > alpha = 0 is equivalent to an ordinary least square. When alpha is very very large, all coefficients are zero. It help to minimizing sum of square of coefficients\n",
    "    \n",
    "- selection\n",
    "> {'cyclic', 'random'}, default='cyclic'\n",
    "If set to ‘random’, a random coefficient is updated every iteration. Often leads to significantly faster convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZQ-kW1_Wxez"
   },
   "outputs": [],
   "source": [
    "# Lasso\n",
    "regressor = Lasso(alpha=0.3, normalize=True)\n",
    "regressor.fit(X_train, Y_train)\n",
    "Y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Lasso with CV\n",
    "reg = LassoCV(cv=5, random_state=0)\n",
    "regressor.fit(X_train, Y_train)\n",
    "Y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRCXdCTvXBHS"
   },
   "source": [
    "## Ridge Regression [L2]\n",
    "\n",
    "Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients with l2 regularization\n",
    "\n",
    "- solver\n",
    "> {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’, ‘lbfgs’}, default=’auto’.\n",
    "lbfgs It can be used only when positive is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkEUPG94XC8r"
   },
   "outputs": [],
   "source": [
    "# Ridge\n",
    "regressor = Ridge(alpha=0.01, normalize=True)\n",
    "regressor.fit(X_train, Y_train)\n",
    "Y_pred = regressor.predict(X_test)\n",
    "\"\"\"\n",
    "    OR\n",
    "\"\"\"\n",
    "regressor = Ridge(alpha=.5, solver=\"cholesky\")\n",
    "regressor.fit(X_train, Y_train)\n",
    "Y_pred = regressor.predict(X_test)\n",
    "\n",
    "\n",
    "# Ridge with CV\n",
    "clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1], cv=5).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yqTHgd1Xb-7"
   },
   "source": [
    "## ElasticNet Regression [L1 + L2]\n",
    "\n",
    "- selection\n",
    "> {'cyclic', 'random'}, default='cyclic'\n",
    "If set to ‘random’, a random coefficient is updated every iteration. Often leads to significantly faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpcNeKbRXjrD"
   },
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "regressor = ElasticNet(alpha=1, l1_ratio=0.5, normalize=False)\n",
    "regressor.fit(X_train, Y_train)\n",
    "Y_pred = regressor.predict(X_test)\n",
    "\n",
    "# ElasticNetCV\n",
    "regr = ElasticNetCV(alpha=1, l1_ratio=0.5, cv=5, random_state=0)\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKqsA_u5dyOH"
   },
   "source": [
    "# SGD Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPLVqmarW85S"
   },
   "outputs": [],
   "source": [
    "# SGDRegressor with Lasso\n",
    "reg = SGDRegressor(penalty=\"l1\")\n",
    "reg.fit(X, y.ravel())\n",
    "\n",
    "# SGDRegressor with Ridge\n",
    "reg = SGDRegressor(penalty=\"l2\")\n",
    "reg.fit(X, y.ravel())\n",
    "\n",
    "# SGDRegressor with ElasticNet\n",
    "reg = SGDRegressor(penalty=\"elasticnet\", l1_ratio=0.5)\n",
    "reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKDgXqropogo"
   },
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "- degree\n",
    "> If a single int is given, it specifies the maximal degree of the polynomial features. If a tuple (min_degree, max_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMc-t5hmpwkr"
   },
   "outputs": [],
   "source": [
    "\"\"\" Find best degree \"\"\"\n",
    "rmses = []\n",
    "degrees = np.arange(1, 10)\n",
    "min_rmse, min_deg = 1e10, 0   # 1e10 = 1*10^10\n",
    "for deg in degrees:\n",
    "    poly_features = PolynomialFeatures(degree=deg, include_bias=False)\n",
    "    x_poly_train = poly_features.fit_transform(X_train)\n",
    "    poly_reg = LinearRegression()\n",
    "    poly_reg.fit(x_poly_train, Y_train)\n",
    "    x_poly_test = poly_features.fit_transform(X_test)\n",
    "    poly_predict = poly_reg.predict(x_poly_test)\n",
    "    poly_rmse = mean_squared_error(Y_test, poly_predict, squared=False)\n",
    "    rmses.append(poly_rmse)\n",
    "    if min_rmse > poly_rmse:\n",
    "        min_rmse = poly_rmse\n",
    "        min_deg = deg\n",
    "\n",
    "print('Best degree {} with RMSE {}'.format(min_deg, min_rmse))\n",
    "\n",
    "\"\"\" Ploting MSE with degree \"\"\"\n",
    "plt.plot(degrees, rmses)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('RMSE')\n",
    "rmses.sort()\n",
    "\n",
    "# traing started\n",
    "poly_features = PolynomialFeatures(degree=min_deg, include_bias=False)\n",
    "X_train = poly_features.fit_transform(X_train)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, Y_train)\n",
    "\n",
    "X_test = poly_features.fit_transform(X_test)\n",
    "Y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN9gmgMbp3rP"
   },
   "source": [
    "# Support Vector Regression\n",
    "\n",
    "- kernel\n",
    "> kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable, default=’rbf’\n",
    "\n",
    "- degree\n",
    "> Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels\n",
    "\n",
    "- C\n",
    "> Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty\n",
    "\n",
    "- epsilon\n",
    "> The larger ϵ is, the larger errors you admit in your solution. By contrast, if ϵ→0+, every error is penalized\n",
    "\n",
    "- gamma\n",
    "> {‘scale’, ‘auto’} or float, default=’scale’\n",
    "1. if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,\n",
    "2. if ‘auto’, uses 1 / n_features.\n",
    "\n",
    "- max_iter\n",
    "> Hard limit on iterations within solver, or -1 for no limit\n",
    "\n",
    "- verbose\n",
    "> if enabled, may not work properly in a multithreaded context. Controls the verbosity when fitting and predicting\n",
    "\n",
    "SVR gives us the flexibility to define how much error is acceptable in our model and will find an appropriate line (or hyperplane in higher dimensions) to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvWI3g_Kp_j_"
   },
   "outputs": [],
   "source": [
    "# kernel = rbf\n",
    "regressor = SVR()\n",
    "params = {'C': 1.0, 'cache_size': 200, 'coef0': 0.0, 'degree': 0, 'epsilon': 0.1, 'gamma': 'scale', \n",
    "          'kernel': 'rbf', 'max_iter': -1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
    "regressor.set_params(**params)\n",
    "\n",
    "# kernel = poly\n",
    "regressor = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "\n",
    "regressor.fit(X_train, Y_train)\n",
    "Y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFViMiEV5wHm"
   },
   "source": [
    "# Decision Tree Regressor\n",
    "\n",
    "- splitter: The strategy used to choose the split at each node. {“best”, “random”}\n",
    "\n",
    "- min_weight_fraction_leaf: The minimum weight fraction of the sum total of weights required to be at a leaf node\n",
    "\n",
    "- ccp_alpha: Greater values of ccp_alpha increase the number of nodes pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfGq7d7T5xJc"
   },
   "outputs": [],
   "source": [
    "regressor = DecisionTreeRegressor()\n",
    "params = {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, \n",
    "          'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, \n",
    "          'min_weight_fraction_leaf': 0.0, 'random_state': 0, 'splitter': 'best'}\n",
    "regressor.set_params(**params)\n",
    "regressor.fit(X_train, Y_train)\n",
    "Y_pred = regressor.predict(X_test)\n",
    "\n",
    "\n",
    "# view tree\n",
    "from sklearn import tree\n",
    "tree.plot_tree(regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nc2jf12b_lud"
   },
   "source": [
    "# Random Forest Regressor\n",
    "\n",
    "- n_estimators\n",
    "> The number of trees in the forest\n",
    "\n",
    "- criterion\n",
    "> The function to measure the quality of a split. \"squared_error\", \"absolute_error\", \"poisson\". default=\"squared_error\"\n",
    "\n",
    "- max_depth\n",
    "> The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples\n",
    "\n",
    "- min_samples_split\n",
    "> The minimum number of samples required to split an internal node\n",
    "\n",
    "- min_samples_leaf\n",
    "> The minimum number of samples required to be at a leaf node. The more you increase the number, the more is the possibility of overfitting\n",
    "\n",
    "- max_features\n",
    "> The number of features to consider when looking for the best split.\n",
    "[“auto”, “sqrt”, “log2”]\n",
    "\n",
    "- bootstrap\n",
    "> Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree\n",
    "\n",
    "- oob_score\n",
    "> Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=True\n",
    "\n",
    "- max_samples\n",
    "> If bootstrap is True, the number of samples to draw from X to train each base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUdgELyt_xeO"
   },
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor()\n",
    "params = {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, \n",
    "          'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, \n",
    "          'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, \n",
    "          'n_jobs': None, 'oob_score': False, 'random_state': 0, 'verbose': 0, 'warm_start': False}\n",
    "regressor.set_params(**params)\n",
    "regressor.fit(X_train, Y_train)\n",
    "Y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocPI-HCVtFAh"
   },
   "source": [
    "# Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vaGKIsAutCij"
   },
   "outputs": [],
   "source": [
    "reg = BaggingRegressor(\n",
    "    base_estimator=SVR(), n_estimators=10, random_state=0\n",
    ")\n",
    "\n",
    "reg.fit(X, y)\n",
    "reg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq9aDarLyjTM"
   },
   "source": [
    "# Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aA0DslkEyn6V"
   },
   "outputs": [],
   "source": [
    "estimators = [\n",
    "('r1', LinearRegression()),\n",
    "('r2', RandomForestRegressor(n_estimators=10, random_state=1))\n",
    "]\n",
    "\n",
    "reg = VotingRegressor(estimators=estimators)\n",
    "\n",
    "reg.fit(X, y)\n",
    "reg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4K1cuXTf0lsf"
   },
   "source": [
    "# Stacking Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1Z-otd1iQdT"
   },
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('lr', RidgeCV()),\n",
    "    ('svr', LinearSVR(random_state=42))\n",
    "]\n",
    "\n",
    "reg = StackingRegressor(\n",
    "    estimators=estimators, final_estimator=RandomForestRegressor(n_estimators=10, random_state=42)\n",
    ")\n",
    "\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro_cskfY7Dxy"
   },
   "source": [
    "# Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Of7endOr_yxL"
   },
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(max_depth=2) \n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "y2 = y - tree_reg1.predict(X) \n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2) \n",
    "tree_reg2.fit(X, y2)\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2) \n",
    "tree_reg3.fit(X, y3)\n",
    "\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "\n",
    "\"\"\"\n",
    "  OR\n",
    "\"\"\"\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0) \n",
    "gbrt.fit(X, y)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    In order to find the optimal number of trees\n",
    "\"\"\"\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120) \n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)] \n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators) \n",
    "gbrt_best.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "     early stopping\n",
    "\"\"\"\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error \n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1 \n",
    "        if error_going_up == 5: \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKmmj0KxiBbb"
   },
   "source": [
    "# XGB Regressor [Extreme Gradient Boosting]\n",
    "\n",
    "> XGBoost is an implementation of Gradient Boosted decision trees\n",
    "\n",
    "- booster\n",
    "> gbtree, gblinear or dart\n",
    "\n",
    "- objective\n",
    "> binary:logistic, reg:linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5o82vTWkODP"
   },
   "outputs": [],
   "source": [
    "xgb_reg = XGBRegressor(objective ='reg:linear', n_estimators = 10, seed = 123)\n",
    "xgb_reg.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDYFzxPykqD9"
   },
   "outputs": [],
   "source": [
    "# Train and test set are converted to DMatrix objects,\n",
    "# as it is required by learning API.\n",
    "train_dmatrix = xg.DMatrix(data = train_X, label = train_y)\n",
    "test_dmatrix = xg.DMatrix(data = test_X, label = test_y)\n",
    "  \n",
    "# Parameter dictionary specifying base learner\n",
    "param = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
    "  \n",
    "xgb_r = xg.train(params = param, dtrain = train_dmatrix, num_boost_round = 10)\n",
    "pred = xgb_r.predict(test_dmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGVfuFBNBGnK"
   },
   "outputs": [],
   "source": [
    "# read in data\n",
    "dtrain = xg.DMatrix('demo/data/agaricus.txt.train')\n",
    "dtest = xg.DMatrix('demo/data/agaricus.txt.test')\n",
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "bst = xg.train(param, dtrain, num_round)\n",
    "# make prediction\n",
    "preds = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec4EKW1M-fue"
   },
   "source": [
    "# Adaptive Boosting\n",
    "\n",
    "#### Params\n",
    "\n",
    "- base_estimator: If None, then the base estimator is DecisionTreeRegressor initialized with max_depth=1\n",
    "- algorithm: {‘SAMME’, ‘SAMME.R’} \n",
    "If ‘SAMME.R’ then use the SAMME.R real boosting algorithm. base_estimator must support calculation of class probabilities. If ‘SAMME’ then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RW-9rOWR-vX9"
   },
   "outputs": [],
   "source": [
    "regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvI5c3D5qOtE"
   },
   "source": [
    "# LightBGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zgmXRwoqSQ6"
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "model = LGBMRegressor()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFXTdCJkTB2Z"
   },
   "source": [
    "# Cat Boost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbLqEl44TCoC"
   },
   "outputs": [],
   "source": [
    "# pip install catboost\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "categorical_features_indices = np.where(X.dtypes != np.float)[0]\n",
    "\n",
    "model=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')\n",
    "\n",
    "model.fit(X_train, y_train, cat_features=categorical_features_indices, eval_set=(X_test, y_test), plot=True)\n",
    "\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOVbeEbjh5EY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "\n",
    "# initialize data\n",
    "train_data = np.random.randint(0, 100, size=(100, 10))\n",
    "train_label = np.random.randint(0, 1000, size=(100))\n",
    "test_data = np.random.randint(0, 100, size=(50, 10))\n",
    "\n",
    "# initialize Pool\n",
    "train_pool = Pool(train_data, train_label, cat_features=[0,2,5])\n",
    "test_pool = Pool(test_data, cat_features=[0,2,5])\n",
    "\n",
    "# specify the training parameters \n",
    "model = CatBoostRegressor(iterations=2, depth=2, learning_rate=1, loss_function='RMSE')\n",
    "\n",
    "#train the model\n",
    "model.fit(train_pool)\n",
    "preds = model.predict(test_pool)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QD6gsVSB_o6s"
   },
   "source": [
    "# ExtraTreesRegressor\n",
    "\n",
    "> The Extra Trees algorithm works by creating a large number of unpruned decision trees from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5OFUhBx_tzk"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "regressor = ExtraTreesRegressor(n_estimators=100, random_state=0)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m015bm3NQzH8"
   },
   "source": [
    "# Multivariate Adaptive Regression Splines in Python [MARS]\n",
    "\n",
    "- Notes: **\n",
    "1. MARS belongs to the group of regression algorithms used to predict continuous (numerical) target variables\n",
    "2. The algorithm has two stages: the forward stage and the backward stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11786,
     "status": "ok",
     "timestamp": 1641051572941,
     "user": {
      "displayName": "Kuntal Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhmQIdHQwmPKhiStxjoDFgCDfTlLEm6wMLM3_nt=s64",
      "userId": "06445374738296173198"
     },
     "user_tz": -330
    },
    "id": "j0buzrmoQxr0",
    "outputId": "856e9da4-0dc5-4dd6-c34a-ea84f5c88b62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/scikit-learn-contrib/py-earth@v0.2dev\n",
      "  Cloning https://github.com/scikit-learn-contrib/py-earth (to revision v0.2dev) to /tmp/pip-req-build-y9i0ddr8\n",
      "  Running command git clone -q https://github.com/scikit-learn-contrib/py-earth /tmp/pip-req-build-y9i0ddr8\n",
      "  Running command git checkout -b v0.2dev --track origin/v0.2dev\n",
      "  Switched to a new branch 'v0.2dev'\n",
      "  Branch 'v0.2dev' set up to track remote branch 'v0.2dev' from 'origin'.\n",
      "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.7/dist-packages (from sklearn-contrib-py-earth==0.1.0+16.g400f84d) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.7/dist-packages (from sklearn-contrib-py-earth==0.1.0+16.g400f84d) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-contrib-py-earth==0.1.0+16.g400f84d) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->sklearn-contrib-py-earth==0.1.0+16.g400f84d) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->sklearn-contrib-py-earth==0.1.0+16.g400f84d) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->sklearn-contrib-py-earth==0.1.0+16.g400f84d) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "# Install scikit-learn-contrib\n",
    "!pip install git+https://github.com/scikit-learn-contrib/py-earth@v0.2dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjaY-ItCRFH_"
   },
   "outputs": [],
   "source": [
    "from pyearth import Earth\n",
    "\n",
    "regressor = Earth()\n",
    "regressor.fit(X_train, Y_train)\n",
    "Y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-sZ1u4ozzfU"
   },
   "source": [
    "# Principal Components Regression (PCR)\n",
    "\n",
    "PCR is a regression technique which is widely used when you have many independent variables OR multicollinearity exist in your data. It is divided into 2 steps:\n",
    "1. Getting the Principal components\n",
    "2. Run regression analysis on principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01TkrBMw2ilL"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0) \n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.fit_transform(X_test)\n",
    "\n",
    "#train PCR model on training data \n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "#calculate RMSE\n",
    "pred = regr.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh7rh4dB29bM"
   },
   "source": [
    "#  Partial Least Squares (PLS) Regression\n",
    "\n",
    "It is an alternative technique of principal component regression when you have independent variables highly correlated. It is also useful when there are a large number of independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_pdWEv63ee0"
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n",
    "Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n",
    "\n",
    "pls2 = PLSRegression(n_components=2)\n",
    "pls2.fit(X, Y)\n",
    "\n",
    "Y_pred = pls2.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERqGxGrx34M7"
   },
   "source": [
    "# Poisson Regression\n",
    "\n",
    "Poisson regression is used to predict a dependent variable that consists of \"count data\" given one or more independent variables\n",
    "\n",
    "Application of Poisson Regression -\n",
    "\n",
    "1. Predicting the number of calls in customer care related to a particular product\n",
    "2. Estimating the number of emergency service calls during an event\n",
    "\n",
    "> poisson regression assumes the variance equal to its mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ab4uOylq4Hh_"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "clf = PoissonRegressor()\n",
    "X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n",
    "y = [12, 17, 22, 21]\n",
    "clf.fit(X, y)\n",
    "\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LW_bJRaO-bmM"
   },
   "source": [
    "# Cox Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Txyj7pCv__ON"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9v9bZe2-f2Q"
   },
   "source": [
    "# Tobit Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uZqSgE3_-wA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40nOfyCn8lyh"
   },
   "source": [
    "# Negative Binomial Regression\n",
    "\n",
    "Like Poisson Regression, it also deals with count data. The question arises \"how it is different from poisson regression\". The answer is negative binomial regression does not assume distribution of count having variance equal to its mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gyEOUap_-AE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkztaHFz98_O"
   },
   "source": [
    "# Quasi Poisson Regression\n",
    "\n",
    "It is an alternative to negative binomial regression. It can also be used for overdispersed count data. Both the algorithms give similar results, there are differences in estimating the effects of covariates. The variance of a quasi-Poisson model is a linear function of the mean while the variance of a negative binomial model is a quadratic function of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJGe5WL__8Xr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3GZ54Vx6Ld-"
   },
   "source": [
    "# Ordinal Regression\n",
    "\n",
    "Ordinal Regression is used to predict ranked values. In simple words, this type of regression is suitable when dependent variable is ordinal in nature. \n",
    "\n",
    "### Example of ordinal variables\n",
    "1. Survey responses (1 to 6 scale), patient reaction to drug dose (none, mild, severe)\n",
    "2. Predicting the movie rating on a scale of 1 to 5 starts can be considered an ordinal regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6A9AbJw6wQG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHlsZ4VUM2Z6"
   },
   "source": [
    "# Score Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1642593141055,
     "user": {
      "displayName": "Kuntal Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhmQIdHQwmPKhiStxjoDFgCDfTlLEm6wMLM3_nt=s64",
      "userId": "06445374738296173198"
     },
     "user_tz": -330
    },
    "id": "XDxs_wLGMuti",
    "outputId": "f11410c1-98d0-4c61-c995-cb4cae1e254d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_score =  0.9226333502184944\n",
      "test_score =  0.9301891740390946\n",
      "r_2_score =  0.9301891740390946\n",
      "mse =  0.221737051360728\n"
     ]
    }
   ],
   "source": [
    "# Score\n",
    "training_score = regressor.score(X_train, Y_train)\n",
    "test_score = regressor.score(X_test, Y_test)\n",
    "print(\"training_score = \", training_score)\n",
    "print(\"test_score = \", test_score)\n",
    "\n",
    "# R2 Score\n",
    "r_2_score = r2_score(Y_test, Y_pred)\n",
    "print(\"r_2_score = \", r_2_score)\n",
    "\n",
    "# Mean Squared Error\n",
    "mse = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "print(\"mse = \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQOHuHslM7o5"
   },
   "source": [
    "# Compare between Actual and Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1642593155270,
     "user": {
      "displayName": "Kuntal Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhmQIdHQwmPKhiStxjoDFgCDfTlLEm6wMLM3_nt=s64",
      "userId": "06445374738296173198"
     },
     "user_tz": -330
    },
    "id": "BLO9-bTiM9Cp",
    "outputId": "674f8668-e555-4e02-c879-70d7b9af4e6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.31 1.  ]\n",
      " [0.11 0.  ]\n",
      " [2.24 2.  ]\n",
      " [1.41 1.  ]\n",
      " [1.36 1.  ]]\n"
     ]
    }
   ],
   "source": [
    "compare_predict_data = np.concatenate((Y_pred.reshape(len(Y_pred),1), Y_test.reshape(len(Y_test),1)),1)[0:5]\n",
    "print(compare_predict_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPbEHuQgT3fa"
   },
   "source": [
    "# K-fold Cross-Validation\n",
    "\n",
    "- What is Cross-Validation?\n",
    ">Cross-Validation is essentially a technique used to assess how well a model performs on a new independent dataset.\n",
    "The simplest example of cross-validation is when you split your data into three groups: training data, validation data, and testing data, where you see the training data to build the model, the validation data to tune the hyperparameters, and the testing data to evaluate your final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1642593167195,
     "user": {
      "displayName": "Kuntal Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhmQIdHQwmPKhiStxjoDFgCDfTlLEm6wMLM3_nt=s64",
      "userId": "06445374738296173198"
     },
     "user_tz": -330
    },
    "id": "DaY3-hEFT7qH",
    "outputId": "57a92f0d-b3fa-4065-e53d-4677c2429573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.16 %\n",
      "Standard Deviation: 2.15 %\n"
     ]
    }
   ],
   "source": [
    "# Way 1\n",
    "accuracies = cross_val_score(estimator = regressor, X = X_train, y = Y_train, cv = 5)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1641051819419,
     "user": {
      "displayName": "Kuntal Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhmQIdHQwmPKhiStxjoDFgCDfTlLEm6wMLM3_nt=s64",
      "userId": "06445374738296173198"
     },
     "user_tz": -330
    },
    "id": "6LqKdvLlRvZM",
    "outputId": "80259471-4cb2-4901-ed9d-4a25f35c3e38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.71 %\n",
      "Standard Deviation: 4.28 %\n"
     ]
    }
   ],
   "source": [
    "# Way 2\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "accuracies = cross_val_score(regressor, X, y, cv=cv, n_jobs=-1)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3icMYzG1NGzY"
   },
   "source": [
    "## Predict unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8P_N-MvUNFBA"
   },
   "outputs": [],
   "source": [
    "predict_random_data = regressor.predict([[0, 0, 0, 0, 7777777]])\n",
    "print(\"predict is = \", predict_random_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EMCFOup-2XU"
   },
   "source": [
    "# **Model Improvement**  \n",
    "### Using GridSearchCV and RandomizedSearchCV\n",
    "\n",
    "1. https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Okt5R1jh-7pP"
   },
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "# Linear Regression\n",
    "\"\"\" [default params]\n",
    "params = {'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'normalize': 'deprecated', 'positive': False}\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" [All scoring value for Regression]\n",
    "scoring = [\n",
    "  explained_variance, max_error, neg_mean_absolute_error, neg_mean_squared_error,\n",
    "  neg_root_mean_squared_error, neg_mean_squared_log_error, neg_median_absolute_error, r2,\n",
    "  neg_mean_poisson_deviance, neg_mean_gamma_deviance, neg_mean_absolute_percentage_error\n",
    "  ]\n",
    "\"\"\"\n",
    "\n",
    "parameters = {\n",
    "    'fit_intercept':[True,False], \n",
    "    'copy_X':[True, False],\n",
    "    'positive': [True, False],\n",
    "    'normalize': [False]\n",
    "    }\n",
    "grid_search = GridSearchCV(estimator = regressor, param_grid = parameters, scoring = 'explained_variance', cv = 5)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "cvres = grid_search.cv_results_\n",
    "\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"Best Estimator:\", best_estimator)\n",
    "print(\"#\"*20)\n",
    "for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n",
    "    print(np.sqrt(mean_score), params)\n",
    "print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1640766341723,
     "user": {
      "displayName": "Kuntal Samanta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhmQIdHQwmPKhiStxjoDFgCDfTlLEm6wMLM3_nt=s64",
      "userId": "06445374738296173198"
     },
     "user_tz": -330
    },
    "id": "c3iaOxg5EdqG",
    "outputId": "2c728d32-5bb5-4a22-9e08-464ef66733a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 94.31 %\n",
      "Best Parameters: {'C': 0.75, 'gamma': 0.3, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV\n",
    "# Support Vector Regression\n",
    "\"\"\" [default params]\n",
    "  params = {'C': 1.0, 'cache_size': 200, 'coef0': 0.0, 'degree': 3, 'epsilon': 0.1, 'gamma': 'scale', \n",
    "          'kernel': 'rbf', 'max_iter': -1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "All scoring value for Regression\n",
    "\n",
    "scoring = [\n",
    "  explained_variance, max_error, neg_mean_absolute_error, neg_mean_squared_error,\n",
    "  neg_root_mean_squared_error, neg_mean_squared_log_error, neg_median_absolute_error, r2,\n",
    "  neg_mean_poisson_deviance, neg_mean_gamma_deviance, neg_mean_absolute_percentage_error\n",
    "  ]\n",
    "\"\"\"\n",
    "\n",
    "# parameters = {\n",
    "#     'C':[1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "#     'cache_size':[i for i in range(100, 1000, 100)], \n",
    "#     'coef0' : [0.01,10,0.5],\n",
    "#     'degree': [i for i in range(1, 10)],\n",
    "#     'gamma' : ('auto','scale'),\n",
    "#     'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#     'max_iter': [-1],\n",
    "#     'shrinking': [True, False],\n",
    "#     'verbose': [True, False]\n",
    "#     }\n",
    "parameters = [{'C': [0.25, 0.5, 0.75, 1], 'kernel': ['linear']},\n",
    "              {'C': [0.25, 0.5, 0.75, 1], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "grid_search = GridSearchCV(estimator = regressor, param_grid = parameters, scoring = None, cv = 5)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "cvres = grid_search.cv_results_\n",
    "\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"Best Estimator:\", best_estimator)\n",
    "print(\"#\"*20)\n",
    "for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n",
    "    print(np.sqrt(mean_score), params)\n",
    "print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z419AUZmRE0t"
   },
   "outputs": [],
   "source": [
    "# RandomizedSearchCV\n",
    "# my model\n",
    "from scipy.stats import randint\n",
    "parameters = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=regressor, param_distributions=parameters, cv = 5)\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# best_accuracy = random_search.best_params_\n",
    "# best_parameters = random_search.best_score_\n",
    "# print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "# print(\"Best Parameters:\", best_parameters)\n",
    "\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "cvres = grid_search.cv_results_\n",
    "\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"Best Estimator:\", best_estimator)\n",
    "print(\"#\"*20)\n",
    "for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n",
    "    print(np.sqrt(mean_score), params)\n",
    "print(\"#\"*20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMP3Ix4c2JLbzuds01UNP4s",
   "collapsed_sections": [],
   "name": "Copy of 1-Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
